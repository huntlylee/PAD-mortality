# -*- coding: utf-8 -*-
"""
Created on Fri Mar  4 16:51:22 2022

@author: Yike Li, MD. PhD. Vanderbilt University Medical Center
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter


from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.preprocessing import StandardScaler, MinMaxScaler
# from sklearn.decomposition import PCA
# from sklearn.pipeline import Pipeline

from sklearn.linear_model import LogisticRegression
# from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
# from sklearn.utils import class_weight

# from sklearn.naive_bayes import ComplementNB, MultinomialNB
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor

# from keras.models import Sequential
# from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
# from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
# from keras.callbacks import EarlyStopping


from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, f1_score, recall_score, confusion_matrix, precision_recall_curve, average_precision_score, roc_auc_score, make_scorer

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import seaborn as sns

import xgboost as xgb
import lightgbm as lgb
import shap

from sklearn.inspection import permutation_importance
from sklearn import preprocessing
import joblib





''' 1. read and data preprocessing'''

''' 1a. raw data are stored in stata file format by year'''

def read_df():

    df_list = []
    for i in range(2016,2020,1):
        df_list.append(pd.read_stata(r'PATH TO THE FILES\pad_study_{}.dta'.format(i)))         
    df_pad = pd.concat(df_list, ignore_index=True)
    return df_pad.loc[(df_pad['primary_pad'] == 1)]


df_pad = read_df()



''' 1b. get descriptive stat and missing values'''

def descriptive_table(df, show_missing_only = True):
        # zero_val = (df == 0.00).astype(int).sum(axis=0)
        mis_val = df.isna().sum()
        mis_val_percent = 100 * df.isna().sum() / len(df)
        min_val = df.min()
        max_val = df.max()
        mz_table = pd.concat([mis_val, mis_val_percent, min_val, max_val], axis=1)
        mz_table = mz_table.rename(
        columns = {0 : 'Missing Values', 1 : 'Missing Values %', 2 : 'Min Value', 3: 'Max Value'})
        mz_table['Data Type'] = df.dtypes
        
        missing_count = len(mz_table[mz_table.iloc[:,1] != 0])
        
        if show_missing_only == True:
            mz_table = mz_table[mz_table.iloc[:,1] != 0].sort_values('Missing Values %', ascending=False).round(1)
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns and " + str(df.shape[0]) + " Rows.\n"      
               "There are " + str(missing_count) + " columns that have missing values.")
        return mz_table

stat_original = descriptive_table(df_pad, show_missing_only = False)


''' 1c. construtct x and y''' 

target = ['DIED']
features = ['AGE', 'AMONTH', 'AWEEKEND', 'ELECTIVE', 'FEMALE', 'HCUP_ED',  'I10_NDX', 'I10_NPR',
            'PAY1','PL_NCHS', 'RACE', 'ZIPINC_QRTL','HOSP_BEDSIZE','HOSP_LOCTEACH', 'H_CONTRL', 'HOSP_REGION',
            'ynch1',	'ynch2',	'ynch3',	'ynch4',	'ynch5',	'ynch6',	'ynch7',	'ynch8',	'ynch9',	
            'ynch10',	'ynch11',	'ynch12',	'ynch13',	'ynch14',	'ynch15',	'ynch16',	'ynch17',  'amputation', 'pad_proc']


dummy_features = ['AMONTH', 'HCUP_ED', 'HOSP_REGION', 'PAY1','PL_NCHS', 'RACE', 'ZIPINC_QRTL',
                     'HOSP_BEDSIZE','HOSP_LOCTEACH', 'H_CONTRL']

categorize_features = ['I10_NDX', 'I10_NPR']

df_pad = df_pad[df_pad[target[0]].notna()] # with confirmed outcomes



''' 1d. remove features with certain percentage of missing values'''


df_pad = df_pad[[*features, *target]]
df_drop = df_pad.dropna(axis = 0, inplace = False)
print ('Outcome 0: {}({:.1%})\nOutcome 1: {}({:.1%})'.format(
    len(df_drop.loc[df_drop[target[0]] == 0]), 
    len(df_drop.loc[df_drop[target[0]] == 0])/ (len(df_drop.loc[df_drop[target[0]] == 0]) + len(df_drop.loc[df_drop[target[0]] == 1])),
    len(df_drop.loc[df_drop[target[0]] == 1]),
    len(df_drop.loc[df_drop[target[0]] == 1])/ (len(df_drop.loc[df_drop[target[0]] == 0]) + len(df_drop.loc[df_drop[target[0]] == 1])))) #148136:2785


''' 1d. encoding'''

for item in ['ynch1',	'ynch2',	'ynch3',	'ynch4',	'ynch5',	'ynch6',	'ynch7',	'ynch8',	'ynch9',	
            'ynch10',	'ynch11',	'ynch12',	'ynch13',	'ynch14',	'ynch15',	'ynch16',	'ynch17']:
    le = preprocessing.LabelEncoder() 
    df_drop[item] = le.fit_transform(df_drop[item])
    

x = df_drop.loc[:, features]
y = df_drop.loc[:, target]



def categorize_encoder_amonth(row):
    if row == 12 or row == 1 or row == 2:
        return 0
    elif row == 3 or row == 4 or row == 5:
        return 1
    elif row == 6 or row == 7 or row == 8:
        return 2
    elif row == 9 or row == 10 or row == 11:
        return 3
x['AMONTH'] = x['AMONTH'].apply(categorize_encoder_amonth)

def categorize_encoder_ndx(row):
    if 0 <= row <= 10:
        return 0
    elif 11 <= row <= 15:
        return 1
    elif 16 <= row <= 20:
        return 2
    elif row > 20:
        return 3

def categorize_encoder_npr(row):
    if row >= 5:
        return 5
    else: 
        return row
    
x['I10_NDX'] = x['I10_NDX'].apply(categorize_encoder_ndx)
x['I10_NPR'] = x['I10_NPR'].apply(categorize_encoder_npr)



def create_descriptive_table(x):
    
    first_item_list, second_item_list = [], []
    feature_num_list, feature_std_list = [], []
    
    feature_list = list(x.columns)
    
    for feature in feature_list:
        
        a = Counter(x[feature])
        
        if len(a) >= 10: # conintuous:
            first_item_list.append(feature)
            second_item_list.append(feature)
            feature_num_list.append(x[feature].mean())
            feature_std_list.append(x[feature].std())            
        else:
            total_n = sum(a.values())
            for e in list(a.keys()):
                first_item_list.append(feature)
                second_item_list.append(e)
                feature_num_list.append(a[e])
                feature_std_list.append(a[e]/total_n)
    
    df = pd.DataFrame(list(zip(first_item_list, second_item_list, feature_num_list, feature_std_list)),
                      columns=['Features', 'Class', 'Count or mean', '% or SD']
                      )
    
    return df

stat_x = create_descriptive_table(x)



def dummies_encoder(x, category_var_list):
    df_temp = x.copy()
    for var in category_var_list:
        df_temp[var] = df_temp[var].astype(int) #float to in
        df_temp = pd.get_dummies(df_temp, columns=[var], prefix=[var + '_'])
    return df_temp


x = dummies_encoder(x, dummy_features)

stat_pad = descriptive_table(x, show_missing_only = False)



''' 1e. optional: show hist plots'''

def plot_hist_df(df, n_row = 10, n_col = 10): 
    f, axarr = plt.subplots(n_row, n_col,figsize=(6, 4))
    
    for i in range(n_row):
        for j in range(n_col):  
            if i*n_col + j < len(features):
                sns.histplot(df, x = features[i*n_col + j], stat='percent', kde=True, ax = axarr[i,j])
                axarr[i,j].set_ylabel('')
            else:
                sns.histplot(df, x = target[0], stat='percent', kde=True, ax = axarr[i,j])
                axarr[i,j].set_ylabel('')
                
    f.text(0.03, 0.5, 'Percent', va='center', rotation='vertical', size = 14) 
    f.suptitle('Histogram of variables', fontsize = 16)           
    f.tight_layout()
    f.subplots_adjust(top=0.92, left = 0.1)
    plt.show()  

plot_hist_df(df_drop, n_row = 5, n_col = 8)


''' 1f. check correlation'''

def correlation_heatmap(train):
    correlations = train.corr()

    fig, ax = plt.subplots(figsize=(10,10))
    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', cmap="YlGnBu",
                square=True, linewidths=.5, annot=False, cbar_kws={"shrink": .70}
                )
    plt.show()
    
correlation_heatmap(x)






''' 2. Hyperparameter tuning'''

def param_tune_single(ml_model = 'RF', test_param = 'n_estimators', param_range = None):
       
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state = 3214, stratify=y)
    
    train_results = []
    test_results = []
    param_grid = {test_param: param_range}
    
    for param in param_grid[test_param]:
        
        dict_temp = {test_param: param}
        if ml_model == 'RF':
            clf = RandomForestClassifier(random_state = 10, n_jobs=-1, class_weight = 'balanced', criterion='gini', **dict_temp)
              
        elif ml_model == 'XGB':
            clf = xgb.XGBClassifier(subsample=0.5, random_state = 20, n_jobs = -1, eta = 0.1, gamma = 0.1, objective = 'binary:logistic', **dict_temp) # around 70
    
        elif ml_model == 'LGB':
            clf = lgb.LGBMClassifier(subsample=0.5, objective  = 'binary', random_state = 20, n_jobs = -1, learning_rate = 0.1,
                                     silent= True, **dict_temp) # around 70
    
        else:
            clf = LogisticRegression(penalty='l2', class_weight = 'balanced', max_iter = 100000, **dict_temp, solver = 'liblinear') #,
        
        print('training {} at {}...'.format(test_param, param))
        clf.fit(x_train.values, np.ravel(y_train.values))
        train_pred = clf.predict(x_train.values)
        fpr, tpr, thresholds = roc_curve(np.ravel(y_train), train_pred)
        roc_auc = auc(fpr, tpr)
        train_results.append(roc_auc)
        y_pred = clf.predict(x_test.values)
        fpr, tpr, thresholds = roc_curve(np.ravel(y_test), y_pred)
        roc_auc = auc(fpr, tpr)
        test_results.append(roc_auc)
    
    line1, = plt.plot(param_grid[test_param], train_results, 'b', label='Train AUC')
    line2, = plt.plot(param_grid[test_param], test_results, 'r', label='Test AUC')
    plt.legend()
    plt.ylabel('AUC score')
    plt.xlabel(test_param)
    plt.show()
    
param_tune_single(ml_model = 'LR', test_param = 'C', param_range = list(map(lambda x:pow(10, x), list(range(-3, 3, 1)))))




''' 2a. define classifier and grid search target'''
def select_clf(ml_model = 'RF'):
   
        
    if ml_model == 'RF':
        clf = RandomForestClassifier(random_state = 10, n_jobs=-1, class_weight = 'balanced', criterion='gini')
        param_grid = {# 'bootstrap': [True],
                      'max_depth': list(range(4,8,1)),
                      'max_features': list(range(6,37,6)),
                      'min_samples_leaf': list(np.linspace(2e-3, 6e-3, 5, endpoint=True)),
                      'min_samples_split': list(np.linspace(3e-2, 5e-2, 3, endpoint=True)),
                      'n_estimators': [16, 32, 64],
                      # 'class_weight' : ['balanced', 'balanced_subsample'],
                      # 'criterion': ['gini', 'entropy']
                      }

        
    elif ml_model == 'XGB':
        clf = xgb.XGBClassifier(subsample=0.5, random_state = 20, n_jobs = -1, gamma = 0.1, eta = 0.1, objective = 'binary:logistic') # around 70
        param_grid = {#'eta' : [0.1, 0.2, 0.3, 0.4, 0.5] ,
                      'max_depth' : list(range(8, 15, 2)),
                      'min_child_weight' : list(range(4, 8, 1)),
                      # 'gamma' : [ 0.0, 0.1, 0.2, 0.3],
                      'colsample_bytree' : list(np.linspace(0.3, 0.5, 3, endpoint=True)),
                      'n_estimators':  [16, 32, 64]
                      }
        
    elif ml_model == 'LGB':
        clf = lgb.LGBMClassifier(subsample=0.5, objective  = 'binary', random_state = 20, n_jobs = -1, learning_rate = 0.1,
                                 silent= True) 
        param_grid = {#'learning_rate' : [0.1, 0.2, 0.3, 0.4, 0.5],
                      'num_leaves' : list(range(30, 61, 10)),
                      'min_child_samples': list(range(70, 91, 10)),
                      #'gamma' : [0.0, 0.1, 0.2, 0.3],
                      'colsample_bytree' : list(np.linspace(0.3, 0.5, 3, endpoint=True)),
                      'n_estimators':  [16, 32, 64]
                      }
    else:
        clf = LogisticRegression(class_weight = 'balanced', max_iter = 1000000, solver = 'liblinear')
        param_grid = {
                    # 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
                      'C': list(map(lambda x:pow(10, x), list(range(-1, 2, 1))))
                      }

    return clf, param_grid






''' 2b. creat my own scoring function: npv for hyperparamenter tuning'''

def grid_search_wrapper(x_dev, y_dev, clf, scoring = 'PR'):
    """
    fits a GridSearchCV classifier using refit_score for optimization
    prints classifier performance metrics
    """
    if scoring == 'PR':
        
        score_func = 'average_precision'
    
    elif scoring == 'ROC':
        
        score_func = 'roc_auc'
             
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)
    
    grid_search = GridSearchCV(clf, param_grid, scoring = score_func, refit = False,
                           cv=skf, return_train_score=False, n_jobs=-1, verbose = 2)
    
    grid_search.fit(x_dev, np.ravel(y_dev))

    print('Best params for {}'.format(scoring))
    print(grid_search.best_params_)


    return grid_search


# on the dev set, use 5 fold cross validation to get the AUC score, optimal threshold and max-NPV threshold

def train_test_model(x_train, y_train, x_val, ml_model):

    ml_model.fit(x_train, np.ravel(y_train))
    
    pred_prob = ml_model.predict_proba(x_val)
    
    return pred_prob


def adjusted_classes(y_scores, t):
    """
    This function adjusts class predictions based on the prediction threshold (t).
    Will only work for binary classification problems.
    """
    return [1 if y >= t else 0 for y in y_scores]



''' execution of step 2: to get the best hyperparameters via GridSearchCV on the whole dataset'''


clf, param_grid = select_clf('LGB')
grid_search_result = grid_search_wrapper(x, y, clf, scoring = 'ROC') # tune hyperparameters to maximize scoring
print(grid_search_result.best_params_)
grid_search_clf = grid_search_result.best_estimator_
    

''' Best hyperparameters: DIED
    Random Forest: {'max_depth': 7, 'max_features': 12, 'min_samples_leaf': 0.002, 'min_samples_split': 0.03, 'n_estimators': 64}
    Logistic regression: {'C': 0.1}
    XGB: {'colsample_bytree': 0.5, 'max_depth': 8, 'min_child_weight': 5, 'n_estimators': 64}
    LGB: {'colsample_bytree': 0.3, 'min_child_samples': 90, 'n_estimators': 64, 'num_leaves': 30}
'''



''' 3. refit the algorithms with the best hyperparameters to the development set
    get the 95% threshold'''

def train_clf(x_dev, y_dev, ml_model = 'SVM'):
    
        
    if ml_model == 'RF':
        clf = RandomForestClassifier(random_state = 10, n_jobs=-1, class_weight = 'balanced', criterion='gini',
                                     max_depth = 7, max_features = 12, min_samples_leaf = 0.002, min_samples_split = 0.03,
                                     n_estimators = 64)            
    else:
        clf = LogisticRegression(penalty='l2',class_weight = 'balanced', max_iter = 100000, C = 0.1, solver = 'liblinear')

    x_train,x_val,y_train,y_val = train_test_split(x_dev, y_dev,test_size = 0.2, random_state = 10, shuffle = True, stratify = y_dev)    

    clf.fit(x_train, np.ravel(y_train))
 
    pred_score = clf.predict_proba(x_val)[:,1]
              
    fpr, tpr, auc_thresholds = roc_curve(y_val, pred_score)
    
    balanced_idx = np.argmax(tpr - fpr)
    balanced_threshold = auc_thresholds[balanced_idx] 

    return clf, balanced_threshold


    
    
def get_metrics_testset(y_true, y_scores, b_threshold):
    
    
    fpr, tpr, auc_thresholds = roc_curve(y_true, y_scores)
    
    roc_score = auc(fpr, tpr)
    
    ap = average_precision_score(y_true, y_scores)
                 
    y_pred_lbl = adjusted_classes(y_scores, b_threshold)

    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_lbl).ravel()
    
    accuracy = (tp+tn)/len(y_pred_lbl)

    tpr = tp / (tp+fn)
    tnr = tn / (tn+fp)
                   
    pre = tp / (tp+fp)

    print('\nAt threshold {:.4f}\n'.format(b_threshold))
    print(pd.DataFrame(np.array([tp,fp,fn,tn]).reshape(2,2),
                       columns=['pos', 'neg'], 
                       index=['pred_pos', 'pred_neg']))
    
    print('\nAccuracy: {:.4f}\nSensitivity: {:.4f}\nSpecificity: {:.4f}\nPrecision: {:.4f}\nROC: {:.4f}\nAP: {:.4f}'.format(
            accuracy, tpr, tnr, pre, roc_score, ap))

    return accuracy, tpr, tnr, pre, roc_score, ap


''' Step 4. To get the permutation importance score'''

def plot_feature_importance(df, top = 5, title_text = None):
    

    if top is not None:   
        df_fea = df.sort_values('Mean').iloc[-top:]
    else:
        df_fea = df.sort_values('Mean')
        
 
    ax = df_fea.plot.barh(y='Mean', xerr = 'SEM',  color='#86bf91', fontsize = 16)
    
    # Despine
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    
    # Switch off ticks
    ax.tick_params(axis="both", which="both", bottom="off", top="off", labelbottom="on", left="off", right="off", labelleft="on")
    
    # Draw vertical axis lines
    vals = ax.get_xticks()
    for tick in vals:
        ax.axvline(x=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)
        
    ax.legend().set_visible(False)
    
    # Set x-axis label
    ax.set_xlabel("Feature importance score", labelpad=20, weight='bold', size=18)
    
    # Set y-axis label
    ax.set_ylabel("Feautures", labelpad=20, weight='bold', size=16)
    
    ax.set_title(title_text + ' - ' + ml_type, size = 20)
    


''' execution of step 3 and 4 on Logistic regression and Random forests models'''

ml_type = 'LR'

skf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 12345)

thresh_list, acc_list, tpr_list, tnr_list, pre_list, auc_list, ap_list= [], [], [], [], [], [], []

feature_importance_list = []

kfold = 0

for train_index, test_index in skf.split(x, y):
    
    x_dev, x_test = x.iloc[train_index], x.iloc[test_index]
    y_dev, y_test = y.iloc[train_index], y.iloc[test_index]
    
    clf, op_threshold = train_clf(x_dev.values, y_dev.values, ml_model = ml_type)
    
    joblib.dump(clf, r"PATH TO TARGET FOLDER\{}_fold{}.pkl".format(ml_type, kfold)) 

    y_test_score = clf.predict_proba(x_test.values)[:,1]

    accuracy, tpr, tnr, pre, roc_score, ap_score = get_metrics_testset(y_test.to_numpy(), y_test_score, op_threshold)
    
    thresh_list.append(op_threshold)
    acc_list.append(accuracy)
    tpr_list.append(tpr)
    tnr_list.append(tnr)
    pre_list.append(pre)
    auc_list.append(roc_score)
    ap_list.append(ap_score)
    
    feature_importance = permutation_importance(clf, x_test.values, y_test.values, scoring='roc_auc', n_repeats=50, random_state=10, n_jobs=-1) # scoring='neg_log_loss'

    result = pd.DataFrame(data = list(zip(thresh_list, acc_list, tpr_list, tnr_list, pre_list, auc_list, ap_list)),
                          columns= ['Threshold', 'Accuracy', 'Recall', 'Specificity', 'Precision', 'ROC', 'AP'])

    feature_importance_list.append(feature_importance.importances_mean)
    
    kfold += 1
    
    
a = np.vstack(feature_importance_list)

df_importance = pd.DataFrame(list(zip(np.mean(a, axis = 0), np.std(a, axis = 0)/np.sqrt(len(a)))),
                             index=list(x_test.columns),
                             columns = ['Mean', 'SEM'])

df_importance.to_csv(r'PATH TO TARGET FOLDER\Permutation score {}-AUC.csv'.format(ml_type), index=True)  

result.to_csv(r'PATH TO TARGET FOLDER\Result {}-AUC.csv'.format(ml_type), index=False)  

plot_feature_importance(df_importance, top = 20, title_text = target[0].upper())



''' XGBoost, Light GBM
    XGB: {'colsample_bytree': 0.5, 'max_depth': 8, 'min_child_weight': 5, 'n_estimators': 64}
    LGB: {'colsample_bytree': 0.3, 'min_child_samples': 90, 'n_estimators': 64, 'num_leaves': 30}
    
'''


def train_clf_new(x_dev, y_dev, ml_model = 'XGB'):

    x_train,x_val,y_train,y_val = train_test_split(x_dev,y_dev,test_size = 0.2, random_state = 10, shuffle = True, stratify = y_dev)    

    if ml_model == 'XGB':
        
        clf = xgb.XGBClassifier(colsample_bytree = 0.5,
                                max_depth = 8,
                                min_child_weight = 5,
                                n_estimators  = 64,
                                learning_rate  = 0.1, 
                                subsample=0.5,
                                gamma = 0.1,
                                random_state = 20,
                                n_jobs = -1,
                                objective = 'binary:logistic'
                                )
            
        clf.fit(x_train, np.ravel(y_train), 
                eval_set = [(x_train, np.ravel(y_train)), (x_val, np.ravel(y_val))], 
                early_stopping_rounds = 5,
                eval_metric='logloss') 
    
        pred_score = clf.predict_proba(x_val, ntree_limit=clf.best_ntree_limit)[:,1]
        
        
    else: 
        clf = lgb.LGBMClassifier(subsample=0.5, objective  = 'binary', random_state = 20, n_jobs = -1, learning_rate = 0.1,
                                 gamma = 0.1, verbose = 1,
                                 colsample_bytree = 0.3,
                                 min_child_samples = 90,
                                 n_estimators = 64, 
                                 num_leaves = 30
                                 )
 
   
        clf.fit(x_train, np.ravel(y_train), 
                eval_set = [(x_train, np.ravel(y_train)), (x_val, np.ravel(y_val))], 
                early_stopping_rounds = 5,
                eval_metric='logloss') #48 rounds
    
        pred_score = clf.predict_proba(x_val)[:,1]
        
        # y_test_score = clf.predict_proba(x_test)[:,1]
        
    explainer = shap.TreeExplainer(clf)
    shap_values = explainer.shap_values(x_test)

    shap.summary_plot(shap_values, x_test, plot_type="bar")    
       
    fpr, tpr, auc_thresholds = roc_curve(y_val, pred_score)
    
    balanced_idx = np.argmax(tpr - fpr)
    balanced_threshold = auc_thresholds[balanced_idx] 
    
  
    return clf, balanced_threshold

ml_type = 'LGB'

skf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 12345)

thresh_list, acc_list, tpr_list, tnr_list, pre_list, auc_list, ap_list= [], [], [], [], [], [], []

feature_importance_list = []

kfold = 0

for train_index, test_index in skf.split(x, y):
    
    x_dev, x_test = x.iloc[train_index], x.iloc[test_index]
    y_dev, y_test = y.iloc[train_index], y.iloc[test_index]
    
    clf, op_threshold = train_clf_new(x_dev.reset_index(drop = True), y_dev.reset_index(drop = True), ml_model = ml_type)
    
    joblib.dump(clf, r"PATH TO TARGET FOLDER\{}_fold{}.pkl".format(ml_type, kfold)) 
    
    
    y_test_score = clf.predict_proba(x_test)[:,1]
    accuracy, tpr, tnr, npv, roc_score, ap_score = get_metrics_testset(y_test.to_numpy(), y_test_score, op_threshold)
    
    thresh_list.append(op_threshold)
    acc_list.append(accuracy)
    tpr_list.append(tpr)
    tnr_list.append(tnr)
    pre_list.append(npv)
    auc_list.append(roc_score)
    ap_list.append(ap_score)

    feature_importance = permutation_importance(clf, x_test, y_test, scoring='roc_auc', n_repeats=50, random_state=10, n_jobs = -1) #scoring='neg_log_loss',

    result = pd.DataFrame(data = list(zip(thresh_list, acc_list, tpr_list, tnr_list, pre_list, auc_list, ap_list)),
                          columns= ['Threshold', 'Accuracy', 'Recall', 'Specificity', 'Precision', 'ROC', 'AP'])

    feature_importance_list.append(feature_importance.importances_mean)
    
    kfold += 1
      
a = np.vstack(feature_importance_list)

df_importance = pd.DataFrame(list(zip(np.mean(a, axis = 0), np.std(a, axis = 0)/np.sqrt(len(a)))),
                             index=list(x_test.columns),
                             columns = ['Mean', 'SEM'])

df_importance.to_csv(r'PATH TO TARGET FOLDER\Permutation score {}-AUC.csv'.format(ml_type), index=True)  

result.to_csv(r'PATH TO TARGET FOLDER\Result {}-AUC.csv'.format(ml_type), index=False)  

plot_feature_importance(df_importance, top = 20, title_text = target[0].upper())


